---
title: What’s DAG got to do with it?
date: 2025-08-08 00:00:00 -04:00
tags:
- consensus
- BFT
author: Ittai Abraham, Neil Giridharan, and Kartik Nayak
---

Since 2018, blockchain research has seen a surge in [DAG-based BFT protocols](https://decentralizedthoughts.github.io/2022-06-28-DAG-meets-BFT/) aimed at achieving higher throughput. These protocols trace back to [Hashgraph, 2018](https://files.hedera.com/hh_whitepaper_v2.2-20230918.pdf?), [Aleph, 2019](https://arxiv.org/abs/1908.05156), the theoretical work of [All You Need is DAG, 2021](https://arxiv.org/abs/2102.08325), and systems papers like [Narwhal](https://arxiv.org/abs/2105.11827) and [Bullshark](https://arxiv.org/abs/2201.05677).

In this post, we highlight **six key aspects** of this body of research:

1. Separation of dispersal and consensus  
2. Liveness of dispersal in asynchrony and combining DACs  
3. Single-disperser BFT vs. multi-disperser BFT  
4. Certified vs. uncertified DAGs  
5. Virtual voting: piggybacking consensus on the DAG for free  
6. Ordering transactions  

## 1. Separation of Dispersal and Consensus

The [separation of consensus and execution](https://www.cs.rochester.edu/meetings/sosp2003/papers/p195-yin.pdf) improves modularity. Here, we focus on separating **dispersal and consensus**: 

1. **Disperse** the data and obtain a short *data availability certificate* (**DAC**) that proves the data can be retrieved, *binding* the DAC to the data.  
2. **Reach consensus** on the DAC.  
3. **Retrieve** the data using the DAC, achieving *totality*.  

This pattern of dispersal and retrieval using a DAC is called [provable verifiable data dissemination](https://decentralizedthoughts.github.io/2024-08-08-vid/) (provable VID). Provable VIDs can be implemented as [provable broadcast](https://decentralizedthoughts.github.io/2022-09-10-provable-broadcast/) or using [error-correcting codes](https://eprint.iacr.org/2020/842) for improved message complexity at the cost of higher latency. The latter approach appears essential for achieving [$O(1)$ scaling of blockchains](https://decentralizedthoughts.github.io/2023-09-30-scaling/).

Another option is dispersal via provable [reliable broadcast](https://decentralizedthoughts.github.io/2020-09-19-living-with-asynchrony-brachas-reliable-broadcast/), which obtains totality and removes the need for a separate retrieval protocol. This method requires higher message complexity during dispersal but offers lower overall latency. Message complexity can be improved using message-efficient reliable broadcast protocols that [employ error-correcting codes](https://drops.dagstuhl.de/storage/00lipics/lipics-vol179-disc2020/LIPIcs.DISC.2020.28/LIPIcs.DISC.2020.28.pdf).

## 2. Liveness of Dispersal in Asynchrony and Combining DACs

* **Dispersal is live in asynchrony**: since dispersal is a single-writer object and does not solve consensus, it can make progress even under asynchrony and is not limited by the [FLP lower bound](https://decentralizedthoughts.github.io/2019-12-15-asynchrony-uncommitted-lower-bound/).

* **DACs can be combined**: multiple data availability certificates can be aggregated within a *new* dispersal. This combined dispersal has a small DAC, and its retrieval can recursively fetch all previously dispersed data.

Some systems make no progress during asynchrony because they *lock-step* dispersal with consensus progress. These systems suffer performance degradation when the network becomes synchronous again due to backlog processing. In contrast, systems that separate dispersal from consensus continue dispersing in asynchrony and commit the entire batch once synchrony returns. Many DAG-based protocols and protocols like [Autobahn](https://arxiv.org/abs/2401.10369) follow this approach.

Having seen how dispersal can progress independently of consensus, we now turn to who performs dispersal — a single proposer or multiple concurrent ones.

## 3. Single-Disperser BFT vs. Multi-Disperser BFT

BFT consensus protocols typically have a single proposer (also called primary or leader) per view. Separating consensus from dispersal opens two possibilities:

* In **single-disperser BFT**, the primary is both the sole *proposer* and the sole *disperser* in the view.  
* In **multi-disperser BFT**, multiple parties act as dispersers concurrently. The single proposer then combines multiple DACs.

Examples of multi-disperser BFT include [All You Need is DAG](https://arxiv.org/pdf/2102.08325), [Narwhal](https://arxiv.org/abs/2105.11827), [Bullshark](https://arxiv.org/abs/2201.05677), [Shoal](https://arxiv.org/abs/2306.03058), [Sailfish](https://decentralizedthoughts.github.io/2024-05-23-sailfish/), [Shoal++](https://decentralizedthoughts.github.io/2024-06-12-shoalpp/), [Cordial Miners](https://arxiv.org/abs/2205.09174), [Star](https://eprint.iacr.org/2022/625), [Autobahn](https://arxiv.org/abs/2401.10369), and [Mysticeti](https://arxiv.org/pdf/2310.14821). They demonstrate how multiple dispersers can boost throughput and achieve $O(n)$ amortized communication complexity when each disperser handles a disjoint $O(n)$-sized set of transactions.

A similar throughput boost can be achieved in single-disperser BFT protocols that use error-correcting codes for dispersal (see Figure 1 in the [Pipes](https://eprint.iacr.org/2025/1116.pdf) paper).

### When Do Multiple Dispersers Help?

Multiple dispersers help only if their transaction sets are mostly disjoint. If they overlap heavily, bandwidth is wasted because transactions are dispersed redundantly.

From the client perspective (assuming clients have disjoint transactions), the ideal case is that each client uses a single non-faulty disperser, yielding a real bandwidth boost.

However, if $f$ dispersers are faulty, in the worst case each client must send its transactions to $f+1$ dispersers. Without coordination, this repeats every transaction $f+1$ times, and multiple dispersers may redundantly disperse the same transactions.

## 4. Multi-Disperser BFT: Certified and uncertified Directed Acyclic Graphs (DAGs)

**Certified DAG**: [Aleph](https://arxiv.org/abs/1908.05156) pioneered this approach, where each party waits to receive $n-f$ round $i$ DACs from different dispersers and combines them (along with any new data) into a new round $i+1$ dispersal protocol. Recent works like [Sailfish](https://decentralizedthoughts.github.io/2024-05-23-sailfish/) and [Shoal++](https://decentralizedthoughts.github.io/2024-06-12-shoalpp/) focus on improving latency for certified DAGs.

**Uncertified DAG**: [Hashgraph](https://www.swirlds.com/downloads/SWIRLDS-TR-2016-01.pdf) introduced this pattern in the blockchain space, where each party waits for $n-f$ signed round $i$ messages and combines them (along with new data) to create a signed round $i+1$ message. This *uncertified DAG* pattern is used in [Cordial Miners](https://arxiv.org/abs/2205.09174), [Mysticeti](https://arxiv.org/pdf/2310.14821), and [Mahi-Mahi](https://arxiv.org/pdf/2410.08670). The general idea of converting a causal order into a total order has roots in [20th century research on distributed systems](https://www.sciencedirect.com/science/article/pii/S0890540198927705?ref=pdf_download&fr=RR-2&rr=9871d30bdc7e3175).

### Data Synchronization for DAGs

In DAGs, dispersed data includes not only new data from round $i$ but also data from all prior rounds ($i-1$, $i-2$, etc.). 

If reliable broadcast is not used for dispersal, some parties may lack data from earlier rounds, requiring synchronization to fetch missing data.

For uncertified DAGs, Byzantine parties can attack by selectively sending data to some parties while withholding it from others, forcing synchronization on the critical path. In certified DAGs using DACs, parties avoid synchronization on the critical path but must recursively fetch missing data before execution, potentially requiring multiple network round trips. Certified DAGs shift complexity off the critical path: consensus proceeds on DACs, not raw data, while retrieval can occur lazily or in parallel.

Autobahn exemplifies a system that achieves non-blocking synchronization and avoids recursive synchronization by designing the dispersal protocol so retrieval for arbitrarily large data requires only a single round-trip.

## 5. Virtual Voting: Piggybacking consensus on the DAG without extra messages

*Virtual voting* piggybacks consensus messages onto DAG messages, trading off throughput and latency.

Virtual voting adds no bandwidth overhead. However, it can introduce latency drawbacks: consensus protocols may require different timeouts or triggers (e.g., a message from a designated leader or messages with specific content). Additionally, some virtual voting protocols on certified DAGs may incur extra queuing latency if they "miss the bus."

## 6. Ordering Transactions

Most DAG protocols maintain a structure where every block in a view (round) references a super-majority of blocks from the previous view. Despite multiple dispersers, there is a single proposer (leader) per view. Parties agree on whether the proposer proposed a block in view $v$. Given a sequence of ordered proposer blocks across views, non-proposer blocks can be ordered deterministically among them. The super-majority references from view $v$ to blocks in view $v-1$ ensure that sufficiently many non-proposer blocks from $v-1$ can be ordered before the block from view $v$.

### Acknowledgements

This post follows insightful discussions at the [Dagstuhl seminar on Next-Generation Secure Distributed Computing](https://www.dagstuhl.de/seminars/seminar-calendar/seminar-details/24362). We thank the organizers and participants for their valuable feedback.

We also thank Adi Seredinschi, Nibesh Shrestha, and Francois Garillot for useful feedback on this post.

Your thoughts and comments on [X](https://x.com/ittaia/status/1953865132724912580).
